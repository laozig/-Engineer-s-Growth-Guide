# 12. TiKV

TiKV 是一个开源的、分布式的、事务型的键值（Key-Value）数据库。它由PingCAP公司开发，是构建其旗舰产品——分布式关系型数据库TiDB的底层存储引擎。TiKV的设计深受Google Spanner和HBase的影响，旨在提供**水平扩展性**、**强一致性**和**高可用性**。

TiKV是一个从头开始就为云原生环境设计的现代存储系统，并且是云原生计算基金会（CNCF）的毕业项目。

## TiKV 架构

TiKV的架构与之前讨论的Dynamo或Cassandra有显著不同，它选择的是**强一致性（CP）**而非最终一致性（AP）。其核心组件基于**Raft共识算法**。

![TiKV Architecture](https://tikv.org/images/docs/tikv-architecture.png)
*(图片来源: TiKV.org)*

1.  **Placement Driver (PD)**:
    - **角色**: 整个集群的"大脑"，负责元数据管理和调度。
    - **职责**:
        - **存储元数据**: 记录数据（Region）在TiKV节点上的实时分布情况。
        - **分配事务ID**: 为分布式事务分配全局唯一且单调递增的时间戳。
        - **智能调度**: 监控所有TiKV节点的状态（如存储容量、读写负载），并生成调度指令来移动数据（Region），以实现负载均衡和高可用性。
    - **高可用**: PD本身也是一个集群（通常3或5个节点），通过内嵌的etcd（基于Raft）来保证自身的高可用和数据一致性。

2.  **TiKV Server**:
    - **角色**: 负责存储实际数据的节点。
    - **核心概念 - Region**:
        - 为了实现水平扩展，TiKV将整个Key-Value空间切分成连续的块，每个块称为一个**Region**。
        - 每个Region默认大小约为96MB，它代表了一个左闭右开的键范围 `[StartKey, EndKey)`。
        - **Region是数据复制和负载均衡的基本单位**。
    - **Raft Group**:
        - 每个Region都有多个副本（默认为3个）。这多个副本构成了一个**Raft Group**。
        - 在每个Raft Group中，通过Raft协议选举出一个**Leader**，负责处理该Region的所有读写请求。其他副本（Follower）则从Leader处异步复制日志。
    - **RocksDB**:
        - 每个TiKV节点上的数据都存储在本地的**RocksDB**实例中。RocksDB是一个高性能的、嵌入式的KV存储引擎，负责将数据持久化到磁盘。

## 核心特性

### 1. 水平扩展

TiKV的扩展能力是通过Region的**自动分裂与合并**来实现的。
- **Region分裂 (Split)**: 当一个Region的大小或键的数量超过阈值时，它会自动分裂成两个新的、更小的Region。
- **Region合并 (Merge)**: 如果某个Region的数据被大量删除，导致其非常小，PD会调度将其与相邻的小Region合并。
- **负载均衡**: PD会持续监控所有TiKV节点的负载。如果发现某个节点上的Leader过多（读写热点）或存储空间不足，它会生成调度指令，将某个Region的Leader甚至整个Region的副本迁移到其他更空闲的节点上。

这个自动化的分裂、合并和迁移过程对上层应用是完全透明的，使得TiKV集群可以像积木一样平滑地增加节点来扩展容量和性能。

### 2. 分布式事务

TiKV支持满足ACID特性的分布式事务，这是它与许多NoSQL数据库（如Cassandra）最大的区别之一。它采用的是一个经过优化的、基于Google Percolator模型的**两阶段提交 (2PC)**协议。

- **时间戳预写 (Timestamp Oracle - TSO)**: 所有事务都从PD获取一个全局唯一且单调递增的时间戳，这为实现**快照隔离 (Snapshot Isolation - SI)**级别的一致性提供了基础。
- **两阶段提交 (2PC)**:
    1.  **Prewrite (预写) 阶段**:
        - 客户端选择一个Key作为本次事务的**Primary Key**。
        - 客户端并发地向所有涉及的Key所在的Region Leader发送Prewrite请求，请求中包含了所有要写入的数据和一个指向Primary Key的指针。
        - Leader检查数据是否存在冲突（例如，是否有其他事务已经锁定了这个Key），如果没有冲突，就将数据和锁写入RocksDB。
    2.  **Commit (提交) 阶段**:
        - 如果所有Prewrite都成功，客户端向Primary Key所在的Region Leader发送Commit请求。
        - Leader检查锁并提交Primary Key。
        - 提交成功后，整个事务就宣告成功。其他Secondary Key的提交是**异步**进行的。

这个模型保证了事务的原子性，即使在部分节点故障的情况下也能保证数据的一致性。

## TiKV vs. etcd

TiKV和etcd都使用了Raft算法，但它们的设计目标和应用场景完全不同。

| 特性 | TiKV | etcd |
| --- | --- | --- |
| **设计目标** | 分布式、事务型、**大规模**键值存储。 | 分布式、可靠的**小规模**键值存储，用于服务发现和配置共享。|
| **容量** | 设计用来存储**海量**数据 (TB级别或更高)。 | 通常用于存储**少量**关键元数据 (GB级别)。 |
| **API** | 提供丰富的KV API，包括事务、范围扫描等。 | 提供简单的KV API (Get, Put, Delete, Watch)。 |
| **扩展性** | 通过Region自动分裂实现**水平扩展**。 | 不支持自动分裂，容量和性能扩展受限。 |
| **事务** | 支持完整的分布式ACID事务。 | 支持多键的原子事务（Mini-Transactions）。 |
| **总结** | 一个**数据库**。 | 一个**协调服务**。 |

## 总结

TiKV是一个现代化的、云原生的分布式键值数据库。它通过将Raft共识协议、Region自动分裂、分布式事务等关键技术巧妙地结合在一起，成功地在一个系统中同时提供了**水平扩展性**、**强一致性**和**高可用性**这三个通常被认为难以兼得的特性。

虽然它可以作为一个独立的KV数据库使用，但它最大的价值在于作为构建更上层分布式系统（特别是分布式数据库，如TiDB）的坚实存储底座。 